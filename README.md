# Awesome-CLIP-Zero/Few-shot-Learning
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

ðŸš€ðŸš€ðŸš€ This repository lists some awesome public projects about Zero-shot/Few-shot Learning based on CLIP (Contrastive Language-Image Pre-Training).

## CLIP 
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) [[CODE](https://github.com/openai/CLIP)]
- [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- [Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14
- [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem

## Few-shot Learning
* [CoOP] [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134), IJCV 2022. [[CODE](https://github.com/KaiyangZhou/CoOp)] ![GitHub Org's stars](https://img.shields.io/github/stars/KaiyangZhou%2FCoOp?style=social)
* [CLIP-Adapter] [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf), arXiv 2110. [[CODE](https://github.com/gaopengcuhk/CLIP-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/gaopengcuhk%2FCLIP-Adapter?style=social)

* [CoCoOp] [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557), CVPR 2022. [[CODE](https://github.com/KaiyangZhou/CoOp)] ![GitHub Org's stars](https://img.shields.io/github/stars/KaiyangZhou%2FCoOp?style=social)
* [Tip-Adapter] [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/pdf/2207.09519.pdf), ECCV 2022. [[CODE](https://github.com/gaopengcuhk/Tip-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/gaopengcuhk%2FTip-Adapter?style=social)

* [SuS-X] [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198), ICCV 2023. [[CODE](https://github.com/vishaal27/SuS-X)] ![GitHub Org's stars](https://img.shields.io/github/stars/vishaal27%2FSuS-X?style=social)

* [CaFo] [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/pdf/2303.02151.pdf), CVPR 2023. [[CODE](https://github.com/OpenGVLab/CaFo)] ![GitHub Org's stars](https://img.shields.io/github/stars/OpenGVLab%2FCaFo?style=social)

* [Proto-CLIP] [Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning](https://arxiv.org/abs/2307.03073), arXiv 2307. [[CODE](https://github.com/IRVLUTD/Proto-CLIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/IRVLUTD%2FProto-CLIP?style=social)

## Zero-shot Learning
* [CuPL] Generating customized prompts for zero-shot image classification. [[CODE](https://github.com/sarahpratt/CuPL)] ![GitHub Org's stars](https://img.shields.io/github/stars/sarahpratt%2FCuPL?style=social)

