# Awesome-CLIP-Zero/Few-shot-Learning
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

ðŸš€ðŸš€ðŸš€ This repository lists some awesome public projects about Zero-shot/Few-shot Learning based on CLIP (Contrastive Language-Image Pre-Training).

## CLIP 
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) [[CODE](https://github.com/openai/CLIP)]
- [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- [Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14
- [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem

## Few-shot Learning
* [[CoOP](https://github.com/KaiyangZhou/CoOp)] ![GitHub Org's stars](https://img.shields.io/github/stars/KaiyangZhou%2FCoOp?style=social) [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134), IJCV 2022. 

* [[CLIP-Adapter](https://github.com/gaopengcuhk/CLIP-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/gaopengcuhk%2FCLIP-Adapter?style=social) [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf), arXiv 2110.

* [VT-CLIP] [VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](https://arxiv.org/abs/2112.02399), arXiv 2112.

* [[CoCoOp](https://github.com/KaiyangZhou/CoOp)] ![GitHub Org's stars](https://img.shields.io/github/stars/KaiyangZhou%2FCoOp?style=social) [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557), CVPR 2022.

* [[ProGrad](https://github.com/BeierZhu/Prompt-align)] ![GitHub Org's stars](https://img.shields.io/github/stars/BeierZhu%2FPrompt-align?style=social) [Prompt-aligned Gradient for Prompt Tuning](https://arxiv.org/abs/2205.14865), ICCV 2023.

* [[Tip-Adapter](https://github.com/gaopengcuhk/Tip-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/gaopengcuhk%2FTip-Adapter?style=social) [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/pdf/2207.09519.pdf), ECCV 2022. 

* [[CALIP-FS](https://github.com/ZiyuGuo99/CALIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/ZiyuGuo99%2FCALIP?style=social) [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169), AAAI 2023.

* [[SuS-X](https://github.com/vishaal27/SuS-X)] ![GitHub Org's stars](https://img.shields.io/github/stars/vishaal27%2FSuS-X?style=social) [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198), ICCV 2023. 

* [[Cross-Modal](https://github.com/linzhiqiu/cross_modal_adaptation)] ![GitHub Org's stars](https://img.shields.io/github/stars/linzhiqiu%2Fcross_modal_adaptation?style=social) [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models](https://arxiv.org/pdf/2301.06267.pdf), CVPR 2023.

* [[CaFo](https://github.com/OpenGVLab/CaFo)] ![GitHub Org's stars](https://img.shields.io/github/stars/OpenGVLab%2FCaFo?style=social) [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/pdf/2303.02151.pdf), CVPR 2023. 

* [[APE](https://github.com/yangyangyang127/APE)] ![GitHub Org's stars](https://img.shields.io/github/stars/yangyangyang127%2FAPE?style=social)
 [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](https://arxiv.org/pdf/2304.01195.pdf), ICCV 2023.

* [[Proto-CLIP](https://github.com/IRVLUTD/Proto-CLIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/IRVLUTD%2FProto-CLIP?style=social) [Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning](https://arxiv.org/abs/2307.03073), arXiv 2307. 

## Zero-shot Learning
* [[CALIP](https://github.com/ZiyuGuo99/CALIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/ZiyuGuo99%2FCALIP?style=social) [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169), AAAI 2023.
* [[CuPL](https://github.com/sarahpratt/CuPL)] ![GitHub Org's stars](https://img.shields.io/github/stars/sarahpratt%2FCuPL?style=social) Generating customized prompts for zero-shot image classification. 

## Fine-tuning

* [[WiSE-FT](https://github.com/mlfoundations/wise-ft)] ![GitHub Org's stars](https://img.shields.io/github/stars/mlfoundations%2Fwise-ft?style=social) [Robust fine-tuning of zero-shot models](https://arxiv.org/abs/2109.01903), CVPR 2022.