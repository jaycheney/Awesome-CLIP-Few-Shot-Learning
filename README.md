# Awesome-CLIP-Zero/Few-shot-Learning
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

ðŸš€ðŸš€ðŸš€ This repository lists some awesome public projects about Zero-shot/Few-shot Learning based on CLIP (Contrastive Language-Image Pre-Training).

## CLIP 
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) [[CODE](https://github.com/openai/CLIP)]
- [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- [Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14
- [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem

## Few-shot Learning
* [CoOP] [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134), IJCV, 2022. [[CODE](https://github.com/KaiyangZhou/CoOp)]
* [CLIP-Adapter] [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf), arXiv 2021. [[CODE](https://github.com/gaopengcuhk/CLIP-Adapter)]
* [CoCoOp] [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557), CVPR, 2022. [[CODE](https://github.com/KaiyangZhou/CoOp)]
* [Tip-Adapter] [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/pdf/2207.09519.pdf), ECCV 2022. [[CODE](https://github.com/gaopengcuhk/Tip-Adapter)]
* [CaFo] [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/pdf/2303.02151.pdf), CVPR 2023. [[CODE](https://github.com/OpenGVLab/CaFo)]

## Zero-shot Learning
* [CuPL] Generating customized prompts for zero-shot image classification. [[CODE](https://github.com/sarahpratt/CuPL)]