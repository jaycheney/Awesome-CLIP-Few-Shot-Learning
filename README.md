# Awesome-CLIP-Zero/Few-shot-Learning
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

ðŸš€ðŸš€ðŸš€ This repository lists some awesome public projects about Zero-shot/Few-shot Learning based on CLIP (Contrastive Language-Image Pre-Training).

## CLIP 
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) [[CODE](https://github.com/openai/CLIP)]
- [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- [Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14
- [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem

## Few-shot Learning
* [[CoOP](https://github.com/KaiyangZhou/CoOp)] ![GitHub Org's stars](https://img.shields.io/github/stars/KaiyangZhou%2FCoOp?style=social) [Learning to Prompt for Vision-Language Models](https://arxiv.org/abs/2109.01134), IJCV 2022. 

* [[CLIP-Adapter](https://github.com/gaopengcuhk/CLIP-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/gaopengcuhk%2FCLIP-Adapter?style=social) [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](https://arxiv.org/pdf/2110.04544.pdf), arXiv 2110.

* [VT-CLIP] [VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts](https://arxiv.org/abs/2112.02399), arXiv 2112.

* [[CoCoOp](https://github.com/KaiyangZhou/CoOp)] ![GitHub Org's stars](https://img.shields.io/github/stars/KaiyangZhou%2FCoOp?style=social) [Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2203.05557), CVPR 2022.

* [[ProGrad](https://github.com/BeierZhu/Prompt-align)] ![GitHub Org's stars](https://img.shields.io/github/stars/BeierZhu%2FPrompt-align?style=social) [Prompt-aligned Gradient for Prompt Tuning](https://arxiv.org/abs/2205.14865), ICCV 2023.

* [SgVA-CLIP] [SgVA-CLIP: Semantic-Guided Visual Adapting of Vision-Language Models for Few-Shot Image Classification](https://ieeexplore.ieee.org/abstract/document/10243119), IEEE Transactions on Multimedia 202309.

* [[Tip-Adapter](https://github.com/gaopengcuhk/Tip-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/gaopengcuhk%2FTip-Adapter?style=social) [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/pdf/2207.09519.pdf), ECCV 2022. 

* [[CALIP-FS](https://github.com/ZiyuGuo99/CALIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/ZiyuGuo99%2FCALIP?style=social) [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169), AAAI 2023.

* [[SuS-X](https://github.com/vishaal27/SuS-X)] ![GitHub Org's stars](https://img.shields.io/github/stars/vishaal27%2FSuS-X?style=social) [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198), ICCV 2023. 

* [[Cross-Modal](https://github.com/linzhiqiu/cross_modal_adaptation)] ![GitHub Org's stars](https://img.shields.io/github/stars/linzhiqiu%2Fcross_modal_adaptation?style=social) [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models](https://arxiv.org/pdf/2301.06267.pdf), CVPR 2023.

* [[CaFo](https://github.com/OpenGVLab/CaFo)] ![GitHub Org's stars](https://img.shields.io/github/stars/OpenGVLab%2FCaFo?style=social) [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://arxiv.org/pdf/2303.02151.pdf), CVPR 2023. 

* [[APE](https://github.com/yangyangyang127/APE)] ![GitHub Org's stars](https://img.shields.io/github/stars/yangyangyang127%2FAPE?style=social) [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](https://arxiv.org/pdf/2304.01195.pdf), ICCV 2023.
* [[PRO](https://github.com/mlvlab/RPO)] ![GitHub Org's stars](https://img.shields.io/github/stars/mlvlab%2FRPO?style=social) [Read-only Prompt Optimization for Vision-Language Few-shot Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.pdf), ICCV 2023.

* [[Proto-CLIP](https://github.com/IRVLUTD/Proto-CLIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/IRVLUTD%2FProto-CLIP?style=social) [Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning](https://arxiv.org/abs/2307.03073), arXiv 2307, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 

* [[GraphAdapter](https://github.com/lixinustc/GraphAdapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/lixinustc%2FGraphAdapter
) [GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph
](https://arxiv.org/abs/2309.13625), NeurIPS 2023.

* [[Meta-Adapter](https://github.com/ArsenalCheng/Meta-Adapter)] ![GitHub Org's stars](https://img.shields.io/github/stars/ArsenalCheng%2FMeta-Adapter
) [Meta-Adapter: An Online Few-shot Learner for Vision-Language Model
](https://arxiv.org/pdf/2311.03774), NeurIPS 2023.

* [[CLAP](https://github.com/YichaoCai1/CLAP)] ![GitHub Org's stars](https://img.shields.io/github/stars/YichaoCai1%2FCLAP) [CLAP: Isolating Content from Style through
Contrastive Learning with Augmented Prompts](https://arxiv.org/pdf/2311.16445), ECCV 2024.

* [[MTA](https://github.com/MaxZanella/MTA)] ![GitHub Org's stars](https://img.shields.io/github/stars/MaxZanella%2FMTA) [On the Test-Time Zero-Shot Generalization of Vision-Language Models: Do We Really Need Prompt Learning?](https://openaccess.thecvf.com/content/CVPR2024/papers/Zanella_On_the_Test-Time_Zero-Shot_Generalization_of_Vision-Language_Models_Do_We_CVPR_2024_paper.pdf), CVPR 2024.

* [[DMN](https://github.com/YBZh/DMN)] ![GitHub Org's stars](https://img.shields.io/github/stars/YBZh%2FDMN
) [Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models
](https://arxiv.org/pdf/2403.17589), CVPR 2024.

* [[AMU-Tuning](https://github.com/TJU-sjyj/AMU-Tuning)] ![GitHub Org's stars](https://img.shields.io/github/stars/TJU-sjyj%2FAMU-Tuning) [AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning
](https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_AMU-Tuning_Effective_Logit_Bias_for_CLIP-based_Few-shot_Learning_CVPR_2024_paper.pdf), CVPR 2024.

## Zero-shot Learning
* [[TPT](https://github.com/azshue/TPT)] ![GitHub Org's stars](https://img.shields.io/github/stars/azshue%2FTPT) [Test-Time Prompt Tuning for Zero-shot Generalization in Vision-Language Models
](https://arxiv.org/pdf/2209.07511.pdf), NeurIPS 2022.

* [[ProDA](https://github.com/bbbdylan/proda)] ![GitHub Org's stars](https://img.shields.io/github/stars/bbbdylan%2Fproda) [Prompt Distribution Learning](vhttps://arxiv.org/abs/2205.03340), CVPR 2022.

* [[DiffTPT](https://github.com/chunmeifeng/DiffTPT)] ![GitHub Org's stars](https://img.shields.io/github/stars/chunmeifeng%2FDiffTPT) [Diverse Data Augmentation with Diffusions for Effective
Test-time Prompt Tuning](https://arxiv.org/pdf/2308.06038), ICCV 2023.

* [[MaPLe](https://github.com/muzairkhattak/multimodal-prompt-learning)] ![GitHub Org's stars](https://img.shields.io/github/stars/muzairkhattak%2Fmultimodal-prompt-learning) [MaPLe: Multi-modal Prompt Learning
](https://arxiv.org/abs/2210.03117), CVPR 2023.

* [[PromptSRC](https://github.com/muzairkhattak/PromptSRC)] ![GitHub Org's stars](https://img.shields.io/github/stars/muzairkhattak%2FPromptSRC) [Self-regulating Prompts: Foundational Model Adaptation without Forgetting
](https://arxiv.org/abs/2307.06948), ICCV 2023.

* [[CALIP](https://github.com/ZiyuGuo99/CALIP)] ![GitHub Org's stars](https://img.shields.io/github/stars/ZiyuGuo99%2FCALIP?style=social) [CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention](https://arxiv.org/abs/2209.14169), AAAI 2023.

* [[CuPL](https://github.com/sarahpratt/CuPL)] ![GitHub Org's stars](https://img.shields.io/github/stars/sarahpratt%2FCuPL?style=social) Generating customized prompts for zero-shot image classification. 

## Fine-tuning

* [[WiSE-FT](https://github.com/mlfoundations/wise-ft)] ![GitHub Org's stars](https://img.shields.io/github/stars/mlfoundations%2Fwise-ft?style=social) [Robust fine-tuning of zero-shot models](https://arxiv.org/abs/2109.01903), CVPR 2022.